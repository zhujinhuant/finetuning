# -*- coding: utf-8 -*-

# Conda Installation
# Select either pytorch-cuda=11.8 for CUDA 11.8 or pytorch-cuda=12.1 for CUDA 12.1. If you have mamba, use mamba instead of conda for faster solving. See this Github issue for help on debugging Conda installs.
#
# conda create --name unsloth_env \
#     python=3.10 \
#     pytorch-cuda=<11.8/12.1> \
#     pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers \
#     -y
# conda activate unsloth_env
#
# pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
#
# pip install --no-deps "trl<0.9.0" peft accelerate bitsandbytes



# ä»unslothåº“å¯¼å…¥FastLanguageModelæ¨¡å—
from unsloth import FastLanguageModel
import torch
max_seq_length = 2048 # å¯ä»¥é€‰æ‹©ä»»ä½•å€¼ï¼æˆ‘ä»¬åœ¨å†…éƒ¨è‡ªåŠ¨æ”¯æŒRoPEç¼©æ”¾ï¼ æ¨¡å‹ä¸€æ¬¡å¯ä»¥æ¥å—å¤šå°‘ä¸ªå•è¯æˆ–æ ‡è®°ï¼ˆtokensï¼‰ã€‚
dtype = None # è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹ã€‚å¯¹äºTesla T4, V100ä½¿ç”¨Float16ï¼Œå¯¹äºAmpere+ä½¿ç”¨Bfloat16
load_in_4bit = True # ä½¿ç”¨4ä½é‡åŒ–å‡å°‘å†…å­˜ä½¿ç”¨ã€‚ä¹Ÿå¯ä»¥è®¾ç½®ä¸ºFalseã€‚

# æˆ‘ä»¬æ”¯æŒçš„4ä½é¢„é‡åŒ–æ¨¡å‹ï¼Œä¸‹è½½é€Ÿåº¦æå‡4å€ï¼Œä¸ä¼šå‡ºç°å†…å­˜æº¢å‡ºã€‚
fourbit_models = [
    "unsloth/mistral-7b-v0.3-bnb-4bit",      # æ–°çš„Mistral v3æ¨¡å‹ï¼Œé€Ÿåº¦æå‡2å€ï¼
    "unsloth/mistral-7b-instruct-v0.3-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit",           # Llama-3æ¨¡å‹ï¼Œå¤„ç†15ä¸‡äº¿tokenï¼Œé€Ÿåº¦æå‡2å€ï¼
    "unsloth/llama-3-8b-Instruct-bnb-4bit",
    "unsloth/llama-3-70b-bnb-4bit",
    "unsloth/Phi-3-mini-4k-instruct",        # Phi-3æ¨¡å‹ï¼Œé€Ÿåº¦æå‡2å€ï¼
    "unsloth/Phi-3-medium-4k-instruct",
    "unsloth/mistral-7b-bnb-4bit",
    "unsloth/gemma-7b-bnb-4bit",             # Gemmaæ¨¡å‹ï¼Œé€Ÿåº¦æå‡2.2å€ï¼
] # æ›´å¤šæ¨¡å‹åœ¨ https://huggingface.co/unsloth

# ä»é¢„è®­ç»ƒæ¨¡å‹ä¸­åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "unsloth/llama-3-8b-bnb-4bit", # å¯ä»¥é€‰æ‹©ä»»ä½•æ¨¡å‹ï¼Œä¾‹å¦‚ teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # å¦‚æœä½¿ç”¨å—é™åˆ¶çš„æ¨¡å‹å¦‚meta-llama/Llama-2-7b-hfï¼Œéœ€ä½¿ç”¨token
)

"""æˆ‘ä»¬ç°åœ¨æ·»åŠ LoRAé€‚é…å™¨ï¼Œè¿™æ ·æˆ‘ä»¬åªéœ€è¦æ›´æ–°1%åˆ°10%çš„æ‰€æœ‰å‚æ•°ï¼å¢å¼ºåŸå§‹æ¨¡å‹ """
# 1. q_proj, k_proj, v_proj  æ³¨æ„åŠ›å±‚
#    è¿™ä¸‰ä¸ªæ¨¡å—åˆ†åˆ«å¯¹åº”äº Transformer çš„æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢ï¼ˆQueryï¼‰ã€é”®ï¼ˆKeyï¼‰å’Œå€¼ï¼ˆValueï¼‰æŠ•å½±çŸ©é˜µã€‚åœ¨è‡ªæ³¨æ„åŠ›å±‚ä¸­ï¼Œè¾“å…¥åºåˆ—è¢«è¿™ä¸‰ä¸ªçŸ©é˜µè½¬æ¢ä»¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼š
#    q_proj (Query Projection)ï¼šå°†è¾“å…¥è½¬æ¢ä¸ºæŸ¥è¯¢å‘é‡ï¼Œç”¨äºä¸é”®å‘é‡æ¯”è¾ƒã€‚
#    k_proj (Key Projection)ï¼šå°†è¾“å…¥è½¬æ¢ä¸ºé”®å‘é‡ï¼Œç”¨äºä¸æŸ¥è¯¢å‘é‡æ¯”è¾ƒç”Ÿæˆæ³¨æ„åŠ›åˆ†æ•°ã€‚
#    v_proj (Value Projection)ï¼šå°†è¾“å…¥è½¬æ¢ä¸ºå€¼å‘é‡ï¼Œè¿™äº›å‘é‡åœ¨è®¡ç®—å¾—åˆ°æ³¨æ„åŠ›åˆ†æ•°åå°†è¢«åŠ æƒæ±‚å’Œï¼Œå½¢æˆè¯¥å±‚çš„è¾“å‡ºã€‚
# 2. o_proj  è¿™æ˜¯è¾“å‡ºæŠ•å½±çŸ©é˜µï¼Œç”¨äºå°†æ³¨æ„åŠ›æœºåˆ¶çš„è¾“å‡ºè¿›ä¸€æ­¥è½¬æ¢ä¸ºä¸‹ä¸€å±‚æˆ–ä¸‹ä¸€æ­¥å¤„ç†çš„è¾“å…¥ã€‚
# 3. gate_proj, up_proj, down_proj è¿™äº›ç»„ä»¶ä¸æ˜¯æ ‡å‡† Transformer æ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼Œå®ƒä»¬å¯èƒ½æ˜¯ç‰¹å®šäºå®ç°çš„æ¨¡å—ï¼Œç”¨äºæ§åˆ¶ä¿¡æ¯æµæˆ–å®ç°ç‰¹å®šçš„ç½‘ç»œæ”¹è¿›ã€‚å…·ä½“æ¥è¯´ï¼š
#    gate_proj (Gate Projection)ï¼šå¯èƒ½ç”¨äºå®ç°é—¨æ§æœºåˆ¶ï¼Œç±»ä¼¼äº LSTM æˆ– GRU ä¸­çš„é—¨æ§ï¼Œæ§åˆ¶ä¿¡æ¯çš„ä¼ é€’å’Œé—å¿˜ã€‚
#    up_proj & down_proj (Up and Down Projections)ï¼šè¿™äº›å¯èƒ½æ˜¯ç”¨äºç‰¹æ®Šçš„å‚æ•°åŒ–æŠ€å·§æˆ–ç½‘ç»œæ¶æ„ä¸­çš„ç‰¹å®šåŠŸèƒ½ï¼Œå¦‚åœ¨LoRAä¸­æ‰©å±•å’Œå‹ç¼©ä¿¡æ¯æµçš„çº¿æ€§å˜æ¢ã€‚

model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # é€‰æ‹©ä»»ä½•å¤§äº0çš„æ•°ï¼æ¨èçš„å€¼åŒ…æ‹¬8, 16, 32, 64, 128  r (Rank)ï¼šè®¾å®šLoRAé€‚é…å™¨ä¸­çš„ä½ç§©çŸ©é˜µçš„ç§©ã€‚ç§©è¶Šä½ï¼Œæ”¹åŠ¨çš„å‚æ•°å°±è¶Šå°‘ï¼Œä½†å¯èƒ½ä¼šç‰ºç‰²ä¸€äº›æ¨¡å‹æ€§èƒ½ã€‚
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],  # æŒ‡å®šè¦åº”ç”¨LoRAé€‚é…å™¨çš„æ¨¡å‹ç»„ä»¶ï¼Œ è¿™äº›æ˜¯æ³¨æ„åŠ›æœºåˆ¶ä¸­çš„æŸ¥è¯¢å’Œé”®æŠ•å½±çŸ©é˜µ
    lora_alpha = 16,  # æ§åˆ¶LoRAé€‚é…å™¨çš„å­¦ä¹ ç‡å€å¢å™¨
    lora_dropout = 0, # æ”¯æŒä»»ä½•å€¼ï¼Œä½†0è¢«ä¼˜åŒ–ä½¿ç”¨ dropoutç‡
    bias = "none",    # æ”¯æŒä»»ä½•å€¼ï¼Œä½†"none"è¢«ä¼˜åŒ–ä½¿ç”¨ ï¼Œ æŒ‡å®šæ˜¯å¦åœ¨LoRAé€‚é…å™¨ä¸­ä½¿ç”¨åç½®é¡¹ã€‚
    # [æ–°åŠŸèƒ½] "unsloth"ä½¿ç”¨30%æ›´å°‘çš„VRAMï¼Œé€‚åˆæ›´å¤§çš„æ‰¹æ¬¡å¤§å°
    use_gradient_checkpointing = "unsloth", # çœŸæˆ–"unsloth"ç”¨äºéå¸¸é•¿çš„ä¸Šä¸‹æ–‡
    random_state = 3407,   #ç¡®ä¿æ¨¡å‹å¾®è°ƒè¿‡ç¨‹çš„å¯é‡å¤æ€§ã€‚
    use_rslora = False,  # æˆ‘ä»¬æ”¯æŒæ’åç¨³å®šçš„LoRA
    loftq_config = None, # ä»¥åŠLoftQ
)





"""
æ•°æ®å‡†å¤‡ï¼š
æˆ‘ä»¬ç°åœ¨ä½¿ç”¨æ¥è‡ªyahmaçš„ç¾Šé©¼æ•°æ®é›†ï¼Œè¿™æ˜¯åŸå§‹ç¾Šé©¼æ•°æ®é›†ä¸­52Kçš„è¿‡æ»¤ç‰ˆæœ¬ã€‚æ‚¨å¯ä»¥ç”¨è‡ªå·±çš„æ•°æ®å‡†å¤‡æ›¿æ¢æ­¤ä»£ç æ®µã€‚
[æ³¨æ„]å¦‚æœåªæƒ³åœ¨å®Œæˆå¥å­ä¸Šè¿›è¡Œè®­ç»ƒï¼ˆå¿½ç•¥ç”¨æˆ·è¾“å…¥ï¼‰ï¼Œè¯·é˜…è¯»TRLçš„æ–‡æ¡£[è¿™é‡Œ](https://huggingface.co/docs/trl/sft_trainer#train-on-completions-only)ã€‚
æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„`get_chat_template`å‡½æ•°è·å–æ­£ç¡®çš„èŠå¤©æ¨¡æ¿ã€‚æˆ‘ä»¬æ”¯æŒ`zephyr, chatml, mistral, llama, alpaca, vicuna, vicuna_old`ä»¥åŠæˆ‘ä»¬è‡ªå·±ä¼˜åŒ–çš„`unsloth`æ¨¡æ¿ã€‚
æ³¨æ„ShareGPTä½¿ç”¨`{"from": "human", "value" : "Hi"}`è€Œä¸æ˜¯`{"role": "user", "content" : "Hi"}`ï¼Œæ‰€ä»¥æˆ‘ä»¬ä½¿ç”¨`mapping`æ¥æ˜ å°„å®ƒã€‚
å¯¹äºæ–‡æœ¬å®Œæˆï¼Œå¦‚å°è¯´å†™ä½œï¼Œè¯·å°è¯•è¿™ä¸ª(https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)ã€‚
"""

from unsloth.chat_templates import get_chat_template
alpaca_prompt = """Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.

### Instruction:
{}

### Input:
{}

### Response:
{}"""
EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN
def formatting_prompts_func(examples):
    instructions = examples["instruction"]
    inputs       = examples["input"]
    outputs      = examples["output"]
    texts = []
    for instruction, input, output in zip(instructions, inputs, outputs):
        # Must add EOS_TOKEN, otherwise your generation will go on forever!
        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
        texts.append(text)
    return { "text" : texts, }
pass
from datasets import load_dataset
dataset = load_dataset("yahma/alpaca-cleaned", split = "train")
dataset = dataset.map(formatting_prompts_func, batched = True,)


"""
è®­ç»ƒæ¨¡å‹
ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨Huggingface TRLçš„â€œSFTTrainerâ€ï¼
æ›´å¤šæ–‡æ¡£è¯·ç‚¹å‡»æ­¤å¤„ï¼š[TRL SFTæ–‡æ¡£](https://huggingface.co/docs/trl/sft_trainer). 
æˆ‘ä»¬åšäº†60ä¸ªæ­¥éª¤æ¥åŠ å¿«é€Ÿåº¦ï¼Œä½†ä½ å¯ä»¥ä¸ºå®Œæ•´è¿è¡Œè®¾ç½®`num_train_epochs=1 `ï¼Œå¹¶å…³é—­`max_steps=None `ã€‚æˆ‘ä»¬ä¹Ÿæ”¯æŒTRLçš„DPOTrainerï¼
"""

# SFTTrainerï¼šè¿™æ˜¯ä¸€ä¸ªè®­ç»ƒå·¥å…·ï¼Œç”¨äºå¯¹æ¨¡å‹è¿›è¡Œç›‘ç£å¼å¾®è°ƒã€‚å®ƒæ˜¯åŸºäº Huggingface Transformers çš„ä¸€ä¸ªè®­ç»ƒæ¥å£ã€‚
# TrainingArgumentsï¼šè¿™ä¸ªç±»æä¾›äº†è®¸å¤šé…ç½®é€‰é¡¹ï¼Œç”¨äºæ§åˆ¶è®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬æ‰¹æ¬¡å¤§å°ã€å­¦ä¹ ç‡ã€è®­ç»ƒæ­¥éª¤æ•°ç­‰ã€‚
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth import is_bfloat16_supported


trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # å¯ä»¥ä½¿è®­ç»ƒå¯¹äºçŸ­åºåˆ—åŠ é€Ÿ5å€ã€‚
    args = TrainingArguments(
        per_device_train_batch_size = 2,    #  æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚
        gradient_accumulation_steps = 4,    # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤æ•°ï¼Œç”¨äºåœ¨æ›´æ–°æ¨¡å‹å‰ç§¯ç´¯æ›´å¤šçš„æ¢¯åº¦ï¼Œæœ‰åŠ©äºç¨³å®šè®­ç»ƒã€‚
        warmup_steps = 5,                   # é¢„çƒ­æ­¥éª¤æ•°ï¼Œé€æ¸å¢åŠ å­¦ä¹ ç‡ä»¥é˜²æ¨¡å‹ä¸€å¼€å§‹è®­ç»ƒæ—¶æ›´æ–°å¤ªå‰§çƒˆã€‚
        max_steps = 60,                     # æœ€å¤§è®­ç»ƒæ­¥æ•°ï¼Œè¿™é‡Œè®¾ç½®ä¸º60æ­¥ã€‚
        learning_rate = 2e-4,               # å­¦ä¹ ç‡
        fp16 = not is_bfloat16_supported(), # åˆ¤æ–­æ˜¯å¦æ”¯æŒåŠç²¾åº¦å’Œbfloat16ç²¾åº¦ï¼Œç”¨äºæé«˜è®­ç»ƒé€Ÿåº¦å’Œé™ä½å†…å­˜æ¶ˆè€—ã€‚
        bf16 = is_bfloat16_supported(),     # åˆ¤æ–­æ˜¯å¦æ”¯æŒåŠç²¾åº¦å’Œbfloat16ç²¾åº¦ï¼Œç”¨äºæé«˜è®­ç»ƒé€Ÿåº¦å’Œé™ä½å†…å­˜æ¶ˆè€—ã€‚
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)

#æ˜¾ç¤ºå½“å‰å†…å­˜ç»Ÿè®¡ä¿¡æ¯
gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

# ********
# å¼€å§‹æ¨¡å‹è®­ç»ƒçš„å®é™…æ‰§è¡Œã€‚
# ********
trainer_stats = trainer.train()

# æ˜¾ç¤ºæœ€ç»ˆå†…å­˜å’Œæ—¶é—´ç»Ÿè®¡æ•°æ®
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory         /max_memory*100, 3)
lora_percentage = round(used_memory_for_lora/max_memory*100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.")
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")





"""
æ¨è®º
è®©æˆ‘ä»¬è¿è¡Œæ¨¡å‹ï¼ç”±äºæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯â€œLlama-3â€ï¼Œå› æ­¤ä½¿ç”¨â€œapply_chat_templateâ€ï¼Œå¹¶å°†â€œadd_generation_promptâ€è®¾ç½®ä¸ºâ€œTrueâ€è¿›è¡Œæ¨ç†ã€‚
"""

# è®¾ç½®ä¸ºæ¨ç†æ¨¡å¼ï¼Œæ­¤æ¨¡å¼ç‰¹åˆ«ä¼˜åŒ–äº†æ¨ç†æ€§èƒ½ï¼Œä½¿æ¨ç†é€Ÿåº¦åŠ å¿«ã€‚
FastLanguageModel.for_inference(model) # å¯ç”¨åŸç”ŸåŠ é€Ÿ2å€çš„æ¨ç†
# å‡†å¤‡ä¸€ä¸ªåŒ…å«æ–æ³¢é‚£å¥‘æ•°åˆ—ç»­å†™è¯·æ±‚çš„æ¶ˆæ¯åˆ—è¡¨ï¼Œå¹¶é€šè¿‡æ¨¡æ¿å¤„ç†è¿™äº›æ¶ˆæ¯ï¼Œå°†å…¶è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥ç†è§£çš„æ ¼å¼ã€‚
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")
# ä½¿ç”¨ model.generate æ–¹æ³•è¿›è¡Œæ–‡æœ¬ç”Ÿæˆã€‚
# æ­¤æ–¹æ³•æ ¹æ®æä¾›çš„è¾“å…¥ï¼ˆæ–æ³¢é‚£å¥‘æ•°åˆ—çš„éƒ¨åˆ†åºåˆ—ï¼‰ç”Ÿæˆæ¥ä¸‹æ¥çš„æ•°å­—åºåˆ—ã€‚è¿™é‡ŒæŒ‡å®šç”Ÿæˆæœ€å¤š64ä¸ªæ–°ä»¤ç‰Œï¼Œå¹¶ä½¿ç”¨ç¼“å­˜åŠ é€Ÿç”Ÿæˆè¿‡ç¨‹ã€‚
outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
# å°†ç”Ÿæˆçš„ä»¤ç‰Œåºåˆ—è§£ç æˆå¯è¯»çš„æ–‡æœ¬ã€‚
tokenizer.batch_decode(outputs)

"""
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨TextStreamerè¿›è¡Œè¿ç»­æ¨ç†ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥é€ä¸ªæŸ¥çœ‹ç”Ÿæˆä»¤ç‰Œï¼Œè€Œä¸æ˜¯ç­‰å¾…
"""
# alpaca_prompt = Copied from above
FastLanguageModel.for_inference(model) # Enable native 2x faster inference
inputs = tokenizer(
[
    alpaca_prompt.format(
        "Continue the fibonnaci sequence.", # instruction
        "1, 1, 2, 3, 5, 8", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

from transformers import TextStreamer
text_streamer = TextStreamer(tokenizer)
_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)






"""
è¦å°†æœ€ç»ˆæ¨¡å‹å¦å­˜ä¸ºLoRAé€‚é…å™¨ï¼Œè¯·ä½¿ç”¨Huggingfaceçš„push_To_hubè¿›è¡Œåœ¨çº¿ä¿å­˜ï¼Œæˆ–ä½¿ç”¨save-presetrainedè¿›è¡Œæœ¬åœ°ä¿å­˜ã€‚
è¿™åªä¿å­˜äº†LoRAé€‚é…å™¨ï¼Œè€Œä¸æ˜¯å®Œæ•´å‹å·ã€‚è¦ä¿å­˜åˆ°16ä½æˆ–GGUFï¼Œè¯·å‘ä¸‹æ»šåŠ¨ï¼
"""
model.save_pretrained("lora_model") # Local saving
tokenizer.save_pretrained("lora_model")

"""
ç°åœ¨ï¼Œå¦‚æœä½ æƒ³åŠ è½½æˆ‘ä»¬åˆšåˆšä¿å­˜ç”¨äºæ¨ç†çš„LoRAé€‚é…å™¨ï¼Œè¯·å°†â€œFalseâ€è®¾ç½®ä¸ºâ€œTrueâ€ï¼š
"""
if False:
    from unsloth import FastLanguageModel
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        max_seq_length = max_seq_length,
        dtype = dtype,
        load_in_4bit = load_in_4bit,
    )
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

# alpaca_prompt = You MUST copy from above!

inputs = tokenizer(
[
    alpaca_prompt.format(
        "What is a famous tall tower in Paris?", # instruction
        "", # input
        "", # output - leave this blank for generation!
    )
], return_tensors = "pt").to("cuda")

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
tokenizer.batch_decode(outputs)

"""
æ‚¨è¿˜å¯ä»¥ä½¿ç”¨Hugging Faceçš„â€œAutoModelForPeftCausalLMâ€ã€‚
ä»…åœ¨æœªå®‰è£…â€œunslothâ€æ—¶ä½¿ç”¨æ­¤é€‰é¡¹ã€‚å®ƒå¯èƒ½éå¸¸æ…¢ï¼Œå› ä¸ºä¸æ”¯æŒâ€œ4bitâ€æ¨¡å‹ä¸‹è½½ï¼Œè€ŒUnlothçš„**æ¨ç†é€Ÿåº¦è¦å¿«2å€**ã€‚
"""
if False:
    # I highly do NOT suggest - use Unsloth if possible
    from peft import AutoModelForPeftCausalLM
    from transformers import AutoTokenizer
    model = AutoModelForPeftCausalLM.from_pretrained(
        "lora_model", # YOUR MODEL YOU USED FOR TRAINING
        load_in_4bit = load_in_4bit,
    )
    tokenizer = AutoTokenizer.from_pretrained("lora_model")

"""
ä¸ºVLLMä¿å­˜åˆ°float16
æˆ‘ä»¬è¿˜æ”¯æŒç›´æ¥ä¿å­˜åˆ°`float16`ã€‚ä¸ºfloat16é€‰æ‹©â€œmerged_16bitâ€ï¼Œä¸ºint4é€‰æ‹©â€œmergerd_4bitâ€ã€‚
æˆ‘ä»¬è¿˜å…è®¸ä½¿ç”¨â€œloraâ€é€‚é…å™¨ä½œä¸ºåå¤‡æ–¹æ¡ˆã€‚
ä½¿ç”¨`push_to_hub_merged`ä¸Šä¼ åˆ°æ‚¨çš„Hugging Faceå¸æˆ·ï¼ä½ å¯ä»¥å»https://huggingface.co/settings/tokensä¸ºæ‚¨çš„ä¸ªäººkeyã€‚
"""

# Merge to 16bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_16bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_16bit", token = "")

# Merge to 4bit
if False: model.save_pretrained_merged("model", tokenizer, save_method = "merged_4bit",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "merged_4bit", token = "")

# Just LoRA adapters
if False: model.save_pretrained_merged("model", tokenizer, save_method = "lora",)
if False: model.push_to_hub_merged("hf/model", tokenizer, save_method = "lora", token = "")

"""
GGUF/calla.cppè½¬æ¢
ä¸ºäº†ä¿å­˜åˆ°â€œGGUFâ€/â€œllama.cppâ€ï¼Œæˆ‘ä»¬ç°åœ¨åŸç”Ÿæ”¯æŒå®ƒï¼
æˆ‘ä»¬å…‹éš†â€œllama.cppâ€ï¼Œå¹¶é»˜è®¤å°†å…¶ä¿å­˜ä¸ºâ€œq8_0â€ã€‚æˆ‘ä»¬å…è®¸æ‰€æœ‰åƒ`q4_k_m`è¿™æ ·çš„æ–¹æ³•ã€‚
ä½¿ç”¨`savepretrained_gguf`è¿›è¡Œæœ¬åœ°ä¿å­˜ï¼Œä½¿ç”¨`push_to_hub_gguf'ä¸Šä¼ åˆ°HFã€‚
ä¸€äº›æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼ˆå®Œæ•´åˆ—è¡¨è§æˆ‘ä»¬çš„[Wikié¡µé¢](https://github.com/unslothai/unsloth/wiki#gguf-é‡åŒ–é€‰é¡¹ï¼‰ï¼š
* `q8_0` - å¿«é€Ÿè½¬æ¢ã€‚èµ„æºåˆ©ç”¨ç‡é«˜ï¼Œä½†æ€»ä½“ä¸Šå¯ä»¥æ¥å—ã€‚
* `q4_k_m` - æ¨èã€‚ä½¿ç”¨Q6_Kä½œä¸ºæ³¨æ„åŠ›.wvå’Œå‰é¦ˆ.w2å¼ é‡çš„ä¸€åŠï¼Œå¦åˆ™ä½¿ç”¨Q4_Kã€‚
* `q5_k_m` - æ¨èã€‚ä½¿ç”¨Q6_Kä½œä¸ºæ³¨æ„åŠ›.wvå’Œå‰é¦ˆ.w2å¼ é‡çš„ä¸€åŠï¼Œå¦åˆ™ä½¿ç”¨Q5_Kã€‚
"""

# Save to 8bit Q8_0
if False: model.save_pretrained_gguf("model", tokenizer,)
if False: model.push_to_hub_gguf("hf/model", tokenizer, token = "")

# Save to 16bit GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "f16")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "f16", token = "")

# Save to q4_k_m GGUF
if False: model.save_pretrained_gguf("model", tokenizer, quantization_method = "q4_k_m")
if False: model.push_to_hub_gguf("hf/model", tokenizer, quantization_method = "q4_k_m", token = "")

"""
ç°åœ¨ï¼Œè¯·åœ¨ `llama.cpp` æˆ–åƒ `GPT4All` è¿™æ ·çš„åŸºäºUIçš„ç³»ç»Ÿä¸­ä½¿ç”¨ `model-unsloth.gguf` æ–‡ä»¶æˆ– `model-unsloth-Q4_K_M.gguf` æ–‡ä»¶ã€‚
ä½ å¯ä»¥é€šè¿‡è®¿é—®[è¿™é‡Œ](https://gpt4all.io/index.html)æ¥å®‰è£… GPT4Allã€‚
æˆ‘ä»¬å·²ç»å®Œæˆäº†ï¼å¦‚æœä½ å¯¹Unslothæœ‰ä»»ä½•ç–‘é—®ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ª[Discord](https://discord.gg/u54VK8m8tk)é¢‘é“ï¼
å¦‚æœä½ å‘ç°ä»»ä½•é”™è¯¯æˆ–æƒ³è¦è·å–æœ€æ–°çš„LLMä¿¡æ¯ï¼Œæˆ–éœ€è¦å¸®åŠ©ï¼ŒåŠ å…¥é¡¹ç›®ç­‰ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬çš„Discordï¼
å…¶ä»–ä¸€äº›é“¾æ¥ï¼š
1. Zephyr DPOåŠ é€Ÿ2å€çš„[å…è´¹Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)
2. Llama 7båŠ é€Ÿ2å€çš„[å…è´¹Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)
3. TinyLlamaåŠ é€Ÿ4å€ï¼Œåœ¨1å°æ—¶å†…å®Œæˆå…¨Alpaca 52Kçš„[å…è´¹Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)
4. CodeLlama 34båœ¨Colabä¸Šçš„A100åŠ é€Ÿ2å€[å…è´¹Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)
5. Mistral 7bçš„[å…è´¹Kaggleç‰ˆæœ¬](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)
6. æˆ‘ä»¬è¿˜ä¸ğŸ¤— HuggingFaceåˆä½œå‘è¡¨äº†ä¸€ç¯‡[åšå®¢](https://huggingface.co/blog/unsloth-trl)ï¼Œå¹¶ä¸”æˆ‘ä»¬åœ¨TRL[æ–‡æ¡£](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)ä¸­æœ‰ä»‹ç»ï¼
7. åƒå°è¯´å†™ä½œä¸€æ ·çš„æ–‡æœ¬å®Œæˆ[ç¬”è®°æœ¬](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)
9. Gemma 6ä¸‡äº¿ä»¤ç‰ŒåŠ é€Ÿ2.5å€çš„[å…è´¹Colab](https://colab.research.google.com/drive/10NbwlsRChbma1v55m8LAPYG15uQv6HLo?usp=sharing)

<div class="align-center">
  <a href="https://github.com/unslothai/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/u54VK8m8tk"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://ko-fi.com/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png" width="145"></a></a> å¦‚æœå¯ä»¥ï¼Œè¯·æ”¯æŒæˆ‘ä»¬çš„å·¥ä½œï¼è°¢è°¢ï¼
</div>
"""